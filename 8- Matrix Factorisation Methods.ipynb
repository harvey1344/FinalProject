{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Notebooks 1-8 have explore the K Nearest Neirbough algorithm in its fullest. It has been throughly tested using different parameters to optimise the model for reccomendations. However, other models for reccomdations exists and to create the best predictor these msut be explored. This notebook aims to introduce the SVD alogirthm provided by the surprise libary, an by using the built in grid search method, the algorithm will be tested to the fullest extent to find our optimal tuning. Again, testing will occur on the smaller 100k dataset due to computional power but the final model will be compared on performance on both the 100k and 1m"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "'After much brute force and various failed runs on my laptop. I decided to tranfer the grid search code to seperate py files to be ran on my pc with 8cores. The process was painstaking slow and that was with the limited number of params configurations i chose. If i was to have my own way, I would have liked to perform a wider grid or random grid solutions. However this waas simply not possible. The matrix factorision algorithm grid search and csv of the setups can be found in the respective folder. I do not reccomend running the scripts unless you want your pc to thermal throttle and your energy bill to sky rocket'"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from surprise import Dataset\n",
    "\n",
    "'''After much brute force and various failed runs on my laptop. I decided to tranfer the grid search code to seperate py files to be ran on my pc with 8cores. The process was painstaking slow and that was with the limited number of params configurations i chose. If i was to have my own way, I would have liked to perform a wider grid or random grid solutions. However this waas simply not possible. The matrix factorision algorithm grid search and csv of the setups can be found in the respective folder. I do not reccomend running the scripts unless you want your pc to thermal throttle and your energy bill to sky rocket'''"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The rest of this notebook will load the parameter setup for each respective algorithm. Train and test on the ml-100 and 1m and the provide visualtions to compare the findings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "   n_factors  lr_all  reg_all  n_epochs\n0        250    0.01      0.1        50",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>n_factors</th>\n      <th>lr_all</th>\n      <th>reg_all</th>\n      <th>n_epochs</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>250</td>\n      <td>0.01</td>\n      <td>0.1</td>\n      <td>50</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start by loading the setup for SVD\n",
    "svd_setup= pd.read_csv('./gridSearch/SVD_rmse_best.csv')\n",
    "svd_setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "svd_n_factors=svd_setup['n_factors'][0]\n",
    "svd_lr_all= svd_setup['lr_all'][0]\n",
    "svd_reg_all= svd_setup['reg_all'][0]\n",
    "svd_n_epochs=svd_setup['n_epochs'][0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "#split into validate and test sets\n",
    "\n",
    "data100k = Dataset.load_builtin('ml-100k')\n",
    "raw_ratings= data100k.raw_ratings\n",
    "\n",
    "random.seed(2001)\n",
    "np.random.seed(2001)\n",
    "random.shuffle(raw_ratings)\n",
    "\n",
    "# create threshold for unseen, 80-20\n",
    "cutoff = int(0.8 * len(raw_ratings))\n",
    "A_raw= raw_ratings[:cutoff]\n",
    "B_raw= raw_ratings[cutoff:]\n",
    "\n",
    "# data is now only set A ratings\n",
    "data100k.raw_ratings= A_raw"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unbiased accuracy on 100k=, RMSE: 0.9106\n",
      "Fit time for 100k = 8.259176015853882\n",
      "Test time for 100k =0.41513943672180176\n",
      "Unbiased accuracy on 100k=, RMSE: 0.9106\n",
      "Fit time for 100k = 7.273743152618408\n",
      "Test time for 100k =0.25599217414855957\n"
     ]
    }
   ],
   "source": [
    "#SVD algorithm- the quickest of the bunch on 100k unseen data\n",
    "from surprise.accuracy import rmse\n",
    "import time\n",
    "from surprise import SVD\n",
    "\n",
    "algo= SVD(n_factors=svd_n_factors, lr_all=svd_lr_all, reg_all=svd_reg_all, n_epochs=svd_n_epochs, random_state=1)\n",
    "trainset = data100k.build_full_trainset()\n",
    "start_fit = time.time()\n",
    "algo.fit(trainset)\n",
    "svd_fit_100k= time.time()-start_fit\n",
    "testset = data100k.construct_testset(B_raw)  # testset is now the set B\n",
    "start_predict= time.time()\n",
    "predictions_100k = algo.test(testset)\n",
    "svd_test_100k= time.time()-start_predict\n",
    "print(\"Unbiased accuracy on 100k=,\", end=\" \")\n",
    "svd_rmse_100k= rmse(predictions_100k)\n",
    "print(\"Fit time for 100k = \"+ str(svd_fit_100k))\n",
    "print(\"Test time for 100k =\"+ str(svd_test_100k))\n",
    "\n",
    "svd_100k_stats=np.array([svd_rmse_100k, svd_fit_100k, svd_test_100k])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "   n_factors  lr_all  reg_all  n_epochs\n0        100   0.005      0.1        70",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>n_factors</th>\n      <th>lr_all</th>\n      <th>reg_all</th>\n      <th>n_epochs</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>0.005</td>\n      <td>0.1</td>\n      <td>70</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the setup for SVDpp\n",
    "svdpp_setup= pd.read_csv('./gridSearch/SVDpp_rmse_best.csv')\n",
    "svdpp_setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "svdpp_n_factors=svd_setup['n_factors'][0]\n",
    "svdpp_lr_all= svd_setup['lr_all'][0]\n",
    "svdpp_reg_all= svd_setup['reg_all'][0]\n",
    "svdpp_n_epochs=svd_setup['n_epochs'][0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unbiased accuracy on 100k=, RMSE: 0.9102\n",
      "Fit time for 100k = 483.4269721508026\n",
      "Test time for 100k =4.542009115219116\n"
     ]
    }
   ],
   "source": [
    "from surprise import SVDpp\n",
    "\n",
    "algo= SVDpp(n_factors=svdpp_n_factors, lr_all=svdpp_lr_all, reg_all=svdpp_reg_all, n_epochs=svdpp_n_epochs, random_state=1)\n",
    "trainset = data100k.build_full_trainset()\n",
    "start_fit = time.time()\n",
    "algo.fit(trainset)\n",
    "svdpp_fit_100k= time.time()-start_fit\n",
    "testset = data100k.construct_testset(B_raw)  # testset is now the set 3356\n",
    "start_predict= time.time()\n",
    "predictions_100k = algo.test(testset)\n",
    "svdpp_test_100k= time.time()-start_predict\n",
    "print(\"Unbiased accuracy on 100k=,\", end=\" \")\n",
    "svdpp_rmse_100k= rmse(predictions_100k)\n",
    "print(\"Fit time for 100k = \"+ str(svdpp_fit_100k))\n",
    "print(\"Test time for 100k =\"+ str(svdpp_test_100k))\n",
    "\n",
    "svdpp_100k_stats=np.array([svdpp_rmse_100k, svdpp_fit_100k, svdpp_test_100k])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "   n_factors  lr_bu  lr_bi  reg_bu  reg_bi  n_epochs  biased\n0         20   0.02   0.02    0.05     0.2        70    True",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>n_factors</th>\n      <th>lr_bu</th>\n      <th>lr_bi</th>\n      <th>reg_bu</th>\n      <th>reg_bi</th>\n      <th>n_epochs</th>\n      <th>biased</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>20</td>\n      <td>0.02</td>\n      <td>0.02</td>\n      <td>0.05</td>\n      <td>0.2</td>\n      <td>70</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf_setup= pd.read_csv('./gridSearch/nmf_rmse_best.csv')\n",
    "nmf_setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "from surprise import NMF\n",
    "\n",
    "# extract hyperparameters\n",
    "nmf_n_factors =nmf_setup['n_factors'].iloc[0]\n",
    "nmf_lr_bu =nmf_setup['lr_bu'].iloc[0]\n",
    "nmf_lr_bi =nmf_setup['lr_bi'].iloc[0]\n",
    "nmf_reg_bu =nmf_setup['reg_bu'].iloc[0]\n",
    "nmf_reg_bi =nmf_setup['reg_bi'].iloc[0]\n",
    "nmf_n_epochs =nmf_setup['n_epochs'].iloc[0]\n",
    "nmf_biased =nmf_setup['biased'].iloc[0]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unbiased accuracy on 100k=, RMSE: 1.4630\n",
      "Fit time for 100k = 2.6150035858154297\n",
      "Test time for 100k =0.1320028305053711\n"
     ]
    }
   ],
   "source": [
    "# create NMF algorithm instance with extracted hyperparameters\n",
    "algo= NMF(n_factors=nmf_n_factors,\n",
    "                     lr_bu=nmf_lr_bu,\n",
    "                     lr_bi=nmf_lr_bi,\n",
    "                     reg_bu=nmf_reg_bu,\n",
    "                     reg_bi=nmf_reg_bi,\n",
    "                     n_epochs=nmf_n_epochs,\n",
    "                     biased=nmf_biased, random_state=1)\n",
    "\n",
    "\n",
    "trainset = data100k.build_full_trainset()\n",
    "start_fit = time.time()\n",
    "algo.fit(trainset)\n",
    "nmfb_fit_100k= time.time()-start_fit\n",
    "testset = data100k.construct_testset(B_raw)  # testset is now the set 3356\n",
    "start_predict= time.time()\n",
    "predictions_100k = algo.test(testset)\n",
    "nmfb_test_100k= time.time()-start_predict\n",
    "print(\"Unbiased accuracy on 100k=,\", end=\" \")\n",
    "nmfb_rmse_100k= rmse(predictions_100k)\n",
    "print(\"Fit time for 100k = \"+ str(nmfb_fit_100k))\n",
    "print(\"Test time for 100k =\"+ str(nmfb_test_100k))\n",
    "\n",
    "nmfb_100k_stats=np.array([nmfb_rmse_100k, nmfb_fit_100k, nmfb_test_100k])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The results for this are unexpected and far from the grid search score. The next cell is going to reuse the nmf algorithm but with biased=False"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unbiased accuracy on 100k=, RMSE: 0.9573\n",
      "Fit time for 100k = 2.5959959030151367\n",
      "Test time for 100k =0.12007331848144531\n"
     ]
    }
   ],
   "source": [
    "# create NMF algorithm instance with extracted hyperparameters\n",
    "algo= NMF(n_factors=nmf_n_factors,\n",
    "                     lr_bu=nmf_lr_bu,\n",
    "                     lr_bi=nmf_lr_bi,\n",
    "                     reg_bu=nmf_reg_bu,\n",
    "                     reg_bi=nmf_reg_bi,\n",
    "                     n_epochs=nmf_n_epochs,\n",
    "                     biased=False, random_state=1)\n",
    "\n",
    "\n",
    "trainset = data100k.build_full_trainset()\n",
    "start_fit = time.time()\n",
    "algo.fit(trainset)\n",
    "nmf_fit_100k= time.time()-start_fit\n",
    "testset = data100k.construct_testset(B_raw)  # testset is now the set 3356\n",
    "start_predict= time.time()\n",
    "predictions_100k = algo.test(testset)\n",
    "nmf_test_100k= time.time()-start_predict\n",
    "print(\"Unbiased accuracy on 100k=,\", end=\" \")\n",
    "nmf_rmse_100k= rmse(predictions_100k)\n",
    "print(\"Fit time for 100k = \"+ str(nmf_fit_100k))\n",
    "print(\"Test time for 100k =\"+ str(nmf_test_100k))\n",
    "\n",
    "nmf_100k_stats=np.array([nmf_rmse_100k, nmf_fit_100k, nmf_test_100k])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Interesting results. Possibly further research into parameter tuning required now ?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "#create a data frame for all the data to store and plot later\n",
    "\n",
    "data=pd.DataFrame(columns=['RMSE', 'Fit Time', 'Predict Time'])\n",
    "data.loc[len(data)] = svd_100k_stats\n",
    "data.loc[len(data)] = svdpp_100k_stats\n",
    "data.loc[len(data)] = nmfb_100k_stats\n",
    "data.loc[len(data)] = nmf_100k_stats\n",
    "data.insert(0,'Algorithm', ['SVD', 'SVDpp', 'NMF(bias)', 'NMF(unbias)'])\n",
    "data.to_csv('./algo_data/matrix_factor_100k')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
